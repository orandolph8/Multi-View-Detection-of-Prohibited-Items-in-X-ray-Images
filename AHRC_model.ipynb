{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "train_test_percent = 0.8\n",
    "\n",
    "base_path = './data/processed/'\n",
    "DvXray_path = base_path + \"classification/train/\"\n",
    "DvXray_set = [('DvXray', 'train')]\n",
    "\n",
    "prohibited_item_classes = {'Gun': 0, 'Knife': 1, 'Wrench': 2, 'Pliers': 3, 'Scissors': 4, 'Lighter': 5, 'Battery': 6,\n",
    "                           'Bat': 7, 'Razor_blade': 8, 'Saw_blade': 9, 'Fireworks': 10, 'Hammer': 11,\n",
    "                           'Screwdriver': 12, 'Dart': 13, 'Pressure_vessel': 14}\n",
    "\n",
    "def convert_annotation(image_id, list_file):\n",
    "\n",
    "    with open(DvXray_path + '%s.json'%image_id, 'r', encoding='utf-8') as j:\n",
    "        label = json.load(j)\n",
    "\n",
    "    gt = np.zeros(15, dtype=int)\n",
    "    objs = label['objects']\n",
    "    if objs == 'None':\n",
    "        gt = gt\n",
    "    else:\n",
    "        for obj in objs:\n",
    "            ind = prohibited_item_classes[obj['label']]\n",
    "            gt[ind] = 1\n",
    "\n",
    "    list_file.write(' ' + ','.join([str(a) for a in gt]))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    random.seed(19)\n",
    "\n",
    "    jsonfilepath = os.path.join(DvXray_path)\n",
    "    saveBasePath = os.path.join(base_path, 'classification/split')\n",
    "\n",
    "    temp_file = os.listdir(jsonfilepath)\n",
    "\n",
    "    total_json = []\n",
    "\n",
    "    for js in temp_file:\n",
    "        if js.endswith('.json'):\n",
    "            total_json.append(js)\n",
    "\n",
    "    num = len(total_json)\n",
    "    list = range(num)\n",
    "    tr = int(num * train_test_percent)\n",
    "    train = random.sample(list, tr)\n",
    "\n",
    "    print(\"train size\", tr)\n",
    "\n",
    "    ftrain = open(os.path.join(saveBasePath, 'train.txt'), 'w')\n",
    "    ftest = open(os.path.join(saveBasePath, 'test.txt'), 'w')\n",
    "\n",
    "    for i in list:\n",
    "        name = total_json[i][:-5] + '\\n'\n",
    "        if i in train:\n",
    "            ftrain.write(name)\n",
    "        else:\n",
    "            ftest.write(name)\n",
    "\n",
    "    ftrain.close()\n",
    "    ftest.close()\n",
    "\n",
    "    for name, img_set in DvXray_set:\n",
    "        image_ids = open(os.path.join(saveBasePath, '%s.txt'%img_set), encoding='utf-8').read().strip().split()\n",
    "        list_file = open('%s_%s.txt'%(name, img_set), 'w', encoding='utf-8')\n",
    "\n",
    "        for image_id in image_ids:\n",
    "            list_file.write(DvXray_path+'%s_OL.png'%image_id)\n",
    "\n",
    "            list_file.write(' ')\n",
    "\n",
    "            list_file.write(DvXray_path+'%s_SD.png'%image_id)\n",
    "\n",
    "            convert_annotation(image_id, list_file)\n",
    "\n",
    "            list_file.write('\\n')\n",
    "        list_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from scipy import ndimage\n",
    "\n",
    "def cvtColor(image):\n",
    "    if len(np.shape(image)) == 3 and np.shape(image)[2] == 3:\n",
    "        return image\n",
    "    else:\n",
    "        image = image.convert('RGB')\n",
    "        return image\n",
    "\n",
    "def preprocess_input(image):\n",
    "    image = np.array(image, dtype=np.float32)[:, :, ::-1]\n",
    "    mean = [0.91584104, 0.9297611, 0.939562]\n",
    "    std = [0.22090791, 0.1861283, 0.1651021]\n",
    "    return (image / 255. - mean) / std\n",
    "\n",
    "def adjust_learning_rate(optimizer, shrink_factor):\n",
    "\n",
    "    print(\"\\nDecaying learning rate.\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
    "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
    "\n",
    "\n",
    "def clip_gradient(optimizer, grad_clip):\n",
    "\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
    "\n",
    "def confidence_weighted_view_fusion(ol_output, sd_output):\n",
    "\n",
    "    total = torch.cat([torch.abs(ol_output - 0.5), torch.abs(sd_output - 0.5)], dim=0)\n",
    "    coff = torch.softmax(total, dim=0)\n",
    "    prediction = coff[0] * ol_output + coff[1] * sd_output\n",
    "\n",
    "    return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class data_loader(Dataset):\n",
    "\n",
    "    def __init__(self, annotation_lines, input_shape):\n",
    "        super(data_loader, self).__init__()\n",
    "\n",
    "        self.annotation_lines = annotation_lines\n",
    "        self.length = len(annotation_lines)\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        index = index % self.length\n",
    "\n",
    "        line = self.annotation_lines[index].split()\n",
    "\n",
    "        image_ol = cvtColor(Image.open(line[0]).resize(self.input_shape, Image.BILINEAR))\n",
    "        image_sd = cvtColor(Image.open(line[1]).resize(self.input_shape, Image.BILINEAR))\n",
    "\n",
    "        image_ol = self.get_random_data(image_ol)\n",
    "        image_sd = self.get_random_data(image_sd)\n",
    "\n",
    "        image_ol = np.transpose(preprocess_input(image_ol), (2, 0, 1))\n",
    "        image_sd = np.transpose(preprocess_input(image_sd), (2, 0, 1))\n",
    "\n",
    "        gt = np.array(list(map(int, line[2].split(','))))\n",
    "\n",
    "        return image_ol, image_sd, gt\n",
    "\n",
    "\n",
    "    def get_random_data(self, image, hue=.1, sat=0.7, val=0.4):\n",
    "\n",
    "        image_data = np.array(image, np.uint8)\n",
    "\n",
    "        r = np.random.uniform(-1, 1, 3) * [hue, sat, val] + 1\n",
    "\n",
    "        hue, sat, val = cv2.split(cv2.cvtColor(image_data, cv2.COLOR_RGB2HSV))\n",
    "        dtype = image_data.dtype\n",
    "\n",
    "        x = np.arange(0, 256, dtype=r.dtype)\n",
    "        lut_hue = ((x * r[0]) % 180).astype(dtype)\n",
    "        lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n",
    "        lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n",
    "\n",
    "        image_data = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val)))\n",
    "        image_data = cv2.cvtColor(image_data, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "        return image_data\n",
    "\n",
    "def DvX_dataset_collate(batch):\n",
    "\n",
    "    img_ols, img_sds, gt_s = [], [], []\n",
    "\n",
    "    for img_ol, img_sd, gt in batch:\n",
    "        img_ols.append(img_ol)\n",
    "        img_sds.append(img_sd)\n",
    "        gt_s.append(gt)\n",
    "\n",
    "    img_ols = torch.from_numpy(np.array(img_ols)).type(torch.FloatTensor)\n",
    "    img_sds = torch.from_numpy(np.array(img_sds)).type(torch.FloatTensor)\n",
    "    gt_s = torch.from_numpy(np.array(gt_s)).type(torch.FloatTensor)\n",
    "\n",
    "    return img_ols, img_sds, gt_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class BCELoss(torch.nn.Module):\n",
    "    def __init__(self, reduction='sum'):\n",
    "        super(BCELoss, self).__init__()\n",
    "\n",
    "        self.reduction = reduction\n",
    "        self.loss_fct = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, ol_output, sd_output, gt_s):\n",
    "\n",
    "        ol_bce_loss = self.loss_fct(ol_output, gt_s)\n",
    "\n",
    "        sd_bce_loss = self.loss_fct(sd_output, gt_s)\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = torch.mean(ol_bce_loss + sd_bce_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = torch.sum(ol_bce_loss + sd_bce_loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class AveragePrecisionMeter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AveragePrecisionMeter, self).__init__()\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.scores = torch.FloatTensor(torch.FloatStorage())\n",
    "        self.targets = torch.LongTensor(torch.LongStorage())\n",
    "\n",
    "    def add(self, output, target):\n",
    "\n",
    "        if not torch.is_tensor(output):\n",
    "            output = torch.from_numpy(output)\n",
    "        if not torch.is_tensor(target):\n",
    "            target = torch.from_numpy(target)\n",
    "\n",
    "        if output.dim() == 1:\n",
    "            output = output.view(-1, 1)\n",
    "        else:\n",
    "            assert output.dim() == 2, \\\n",
    "                'wrong output size (should be 1D or 2D with one column \\\n",
    "                per class)'\n",
    "        if target.dim() == 1:\n",
    "            target = target.view(-1, 1)\n",
    "        else:\n",
    "            assert target.dim() == 2, \\\n",
    "                'wrong target size (should be 1D or 2D with one column \\\n",
    "                per class)'\n",
    "\n",
    "        if self.scores.numel() > 0:\n",
    "            assert target.size(1) == self.targets.size(1), \\\n",
    "                'dimensions for output should match previously added examples.'\n",
    "\n",
    "        if self.scores.storage().size() < self.scores.numel() + output.numel():\n",
    "            new_size = math.ceil(self.scores.storage().size() * 1.5)\n",
    "            self.scores.storage().resize_(int(new_size + output.numel()))\n",
    "            self.targets.storage().resize_(int(new_size + output.numel()))\n",
    "\n",
    "        offset = self.scores.size(0) if self.scores.dim() > 0 else 0\n",
    "\n",
    "        self.scores.resize_(offset + output.size(0), output.size(1))\n",
    "        self.targets.resize_(offset + target.size(0), target.size(1))\n",
    "        self.scores.narrow(0, offset, output.size(0)).copy_(output)\n",
    "        self.targets.narrow(0, offset, target.size(0)).copy_(target)\n",
    "\n",
    "    def value(self):\n",
    "\n",
    "        if self.scores.numel() == 0:\n",
    "            return 0\n",
    "\n",
    "        ap = torch.zeros(self.scores.size(1))\n",
    "\n",
    "        for k in range(self.scores.size(1)):\n",
    "\n",
    "            scores = self.scores[:, k]\n",
    "            targets = self.targets[:, k]\n",
    "\n",
    "            ap[k] = AveragePrecisionMeter.average_precision(scores, targets)\n",
    "        return ap\n",
    "\n",
    "    @staticmethod\n",
    "    def average_precision(output, target):\n",
    "\n",
    "        sorted, indices = torch.sort(output, dim=0, descending=True)\n",
    "\n",
    "        pos_count = 0.\n",
    "        total_count = 0.\n",
    "        precision_at_i = 0.\n",
    "        for i in indices:\n",
    "            label = target[i]\n",
    "            if label == 0:\n",
    "                total_count += 1\n",
    "            if label == 1:\n",
    "                pos_count += 1\n",
    "                total_count += 1\n",
    "            if label == 1:\n",
    "                precision_at_i += pos_count / total_count\n",
    "        precision_at_i /= (pos_count + 1e-10)\n",
    "        return precision_at_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU6'>.\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "9.498G\n",
      "34.882M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "# class h_swish(nn.Module):\n",
    "#     def __init__(self, inplace=True):\n",
    "#         super(h_swish, self).__init__()\n",
    "#         self.relu = nn.ReLU6(inplace=inplace)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         return x * self.relu(x + 3) / 6\n",
    "\n",
    "class r_func(nn.Module):\n",
    "\n",
    "    def __init__(self, int_c, out_c, reduction=32):\n",
    "\n",
    "        super(r_func, self).__init__()\n",
    "\n",
    "        self.pool_h = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        def _make_basic(input_dim, output_dim, kernel_size, stride, padding):\n",
    "\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_dim, output_dim, kernel_size, stride,\n",
    "                          padding),\n",
    "                nn.BatchNorm2d(output_dim))\n",
    "        # self.act = h_swish()\n",
    "        self.act = nn.ReLU6(inplace=True)\n",
    "        self.dcov = _make_basic(int_c, (out_c // reduction), kernel_size=1, stride=1, padding=0)\n",
    "        self.conv_h = nn.Conv2d((out_c // reduction), out_c, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, h_x, l_x, sig):\n",
    "\n",
    "        B, _, H, W = l_x.size()\n",
    "        m_x = self.pool_h(h_x)\n",
    "        m_x = F.interpolate(m_x, size=(1, W), mode='bilinear')\n",
    "        m_x = self.act(self.dcov(m_x))\n",
    "        m_out = l_x * (self.conv_h(m_x).sigmoid())\n",
    "        sig = sig.reshape((B, 1, 1, 1))\n",
    "        out = l_x + sig * m_out\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class AHCR(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=15):\n",
    "\n",
    "        super(AHCR, self).__init__()\n",
    "\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        for item in resnet.children():\n",
    "            if isinstance(item, nn.BatchNorm2d):\n",
    "                item.affine = False\n",
    "\n",
    "        self.features = nn.Sequential(resnet.conv1,\n",
    "                                      resnet.bn1,\n",
    "                                      resnet.relu,\n",
    "                                      resnet.maxpool)\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "\n",
    "        self.cov4_ol = nn.Conv2d(2048, 2048, kernel_size=1, stride=1)\n",
    "        self.cov4_sd = nn.Conv2d(2048, 2048, kernel_size=1, stride=1)\n",
    "\n",
    "        self.cov3_ol = nn.Conv2d(1024, 1024, kernel_size=1, stride=1)\n",
    "        self.cov3_sd = nn.Conv2d(1024, 1024, kernel_size=1, stride=1)\n",
    "\n",
    "        self.cov2_ol = nn.Conv2d(512, 512, kernel_size=1, stride=1)\n",
    "        self.cov2_sd = nn.Conv2d(512, 512, kernel_size=1, stride=1)\n",
    "\n",
    "        self.ol_r_sd_1 = r_func(2048, 1024)\n",
    "        self.sd_r_ol_1 = r_func(2048, 1024)\n",
    "\n",
    "        self.ol_r_sd_2 = r_func(1024, 512)\n",
    "        self.sd_r_ol_2 = r_func(1024, 512)\n",
    "\n",
    "        self.po1 = nn.AvgPool2d(7, stride=1)\n",
    "        self.po2 = nn.AvgPool2d(14, stride=1)\n",
    "        self.po3 = nn.AvgPool2d(28, stride=1)\n",
    "\n",
    "        self.fc1_ol = nn.Linear(2048, num_classes)\n",
    "        self.fc1_sd = nn.Linear(2048, num_classes)\n",
    "\n",
    "        self.fc2_ol = nn.Linear(1024, num_classes)\n",
    "        self.fc2_sd = nn.Linear(1024, num_classes)\n",
    "\n",
    "        self.fc3_ol = nn.Linear(512, num_classes)\n",
    "        self.fc3_sd = nn.Linear(512, num_classes)\n",
    "\n",
    "        self.cos_similarity = nn.CosineSimilarity(dim=1, eps=1e-8)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "\n",
    "        for name, param in self.named_parameters():\n",
    "\n",
    "            if name in ('cov4_ol.weight', 'cov4_sd.weight', 'cov3_ol.weight', 'cov3_sd.weight', 'cov2_ol.weight',\n",
    "                        'cov2_sd.weight', 'ol_r_sd_1.dcov.0.weight', 'ol_r_sd_1.conv_h.weight', 'sd_r_ol_1.dcov.0.weight', 'sd_r_ol_1.conv_h.weight',\n",
    "                        'ol_r_sd_2.dcov.0.weight', 'ol_r_sd_2.conv_h.weight', 'sd_r_ol_2.dcov.0.weight', 'sd_r_ol_2.conv_h.weight'):\n",
    "                nn.init.orthogonal_(param)\n",
    "            if name in ('cov4_ol.bias', 'cov4_sd.bias', 'cov3_ol.bias', 'cov3_sd.bias', 'cov2_ol.bias',\n",
    "                        'cov2_sd.bias', 'ol_r_sd_1.dcov.0.bias', 'ol_r_sd_1.conv_h.bias', 'sd_r_ol_1.dcov.0.bias', 'sd_r_ol_1.conv_h.bias',\n",
    "                        'ol_r_sd_2.dcov.0.bias', 'ol_r_sd_2.conv_h.bias', 'sd_r_ol_2.dcov.0.bias', 'sd_r_ol_2.conv_h.bias'):\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "            if name in ('fc1_ol.weight', 'fc1_sd.weight', 'fc2_ol.weight', 'fc2_sd.weight', 'fc3_ol.weight', 'fc3_sd.weight'):\n",
    "                nn.init.normal_(param, 0, 0.01)\n",
    "            if name in ('fc1_ol.bias', 'fc1_sd.bias', 'fc2_ol.bias', 'fc2_sd.bias', 'fc3_ol.bias', 'fc3_sd.bias'):\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, image_ol, image_sd):\n",
    "\n",
    "        bf_ol = self.features(image_ol)\n",
    "        f_l1_o1 = self.layer1(bf_ol)\n",
    "        f_l2_ol = self.layer2(f_l1_o1)\n",
    "        f_l3_ol = self.layer3(f_l2_ol)\n",
    "        f_l4_ol = self.layer4(f_l3_ol)\n",
    "\n",
    "        bf_sd = self.features(image_sd)\n",
    "        f_l1_sd = self.layer1(bf_sd)\n",
    "        f_l2_sd = self.layer2(f_l1_sd)\n",
    "        f_l3_sd = self.layer3(f_l2_sd)\n",
    "        f_l4_sd = self.layer4(f_l3_sd)\n",
    "\n",
    "        out1_ol = self.fc1_ol(self.po1(F.relu(self.cov4_ol(f_l4_ol))).view(f_l4_ol.size(0), -1))\n",
    "        out1_sd = self.fc1_sd(self.po1(F.relu(self.cov4_sd(f_l4_sd))).view(f_l4_sd.size(0), -1))\n",
    "\n",
    "        con_coe1 = F.relu(self.cos_similarity(out1_ol, out1_sd))\n",
    "\n",
    "        out2_m_ol = self.cov3_ol(self.sd_r_ol_1(f_l4_sd, f_l3_ol, con_coe1))\n",
    "        out2_m_sd = self.cov3_sd(self.ol_r_sd_1(f_l4_ol, f_l3_sd, con_coe1))\n",
    "\n",
    "        out2_ol = self.fc2_ol(self.po2(F.relu(out2_m_ol)).view(out2_m_ol.size(0), -1))\n",
    "        out2_sd = self.fc2_sd(self.po2(F.relu(out2_m_sd)).view(out2_m_sd.size(0), -1))\n",
    "\n",
    "        con_coe2 = F.relu(self.cos_similarity(out2_ol, out2_sd))\n",
    "\n",
    "        out3_m_ol = self.cov2_ol(self.sd_r_ol_2(out2_m_sd, f_l2_ol, con_coe2))\n",
    "        out3_m_sd = self.cov2_sd(self.ol_r_sd_2(out2_m_ol, f_l2_sd, con_coe2))\n",
    "\n",
    "        out3_ol = self.fc3_ol(self.po3(F.relu(out3_m_ol)).view(out3_m_ol.size(0), -1))\n",
    "        out3_sd = self.fc3_sd(self.po3(F.relu(out3_m_sd)).view(out3_m_sd.size(0), -1))\n",
    "\n",
    "        ol_output = (out1_ol + out2_ol + out3_ol) / 3.\n",
    "\n",
    "        sd_output = (out1_sd + out2_sd + out3_sd) / 3.\n",
    "\n",
    "        return ol_output, sd_output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    from thop import profile\n",
    "    from thop import clever_format\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AHCR(num_classes=15).to(device)\n",
    "\n",
    "    flops, params = profile(model, (torch.randn(1, 3, 224, 224).to(device), torch.randn(1, 3, 224, 224).to(device)))\n",
    "    flops, params = clever_format([flops, params], '%.3f')\n",
    "    print(flops)\n",
    "    print(params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 124\u001b[39m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mmAP: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[33m'\u001b[39m.format(\u001b[38;5;28mmap\u001b[39m))\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m epoch % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     71\u001b[39m     adjust_learning_rate(optimizer, \u001b[32m0.1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m save_checkpoint(epoch, model, optimizer)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(train_loader, model, criterion, optimizer, epoch)\u001b[39m\n\u001b[32m     95\u001b[39m loss = criterion(ol_output, sd_output, gt_s)\n\u001b[32m     97\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m Loss.append(loss.item())\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grad_clip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Project2/pvenv/lib/python3.14/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Project2/pvenv/lib/python3.14/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Project2/pvenv/lib/python3.14/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model_ResNet import *\n",
    "\n",
    "# from dataset import data_loader, DvX_dataset_collate\n",
    "# from loss_func import BCELoss\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "# from utils import adjust_learning_rate, clip_gradient, confidence_weighted_view_fusion\n",
    "# from get_ap import AveragePrecisionMeter\n",
    "\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Training parameters\n",
    "start_epoch = 0\n",
    "epochs = 30\n",
    "input_shape = [224, 224]\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "grad_clip = 5.\n",
    "print_freq = 100\n",
    "train_annotation_path = 'data/split/train.txt'\n",
    "\n",
    "checkpoint = None\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer):\n",
    "\n",
    "    state = {'epoch': epoch,\n",
    "             'model': model,\n",
    "             'optimizer': optimizer,\n",
    "             }\n",
    "\n",
    "    filename = 'ep%03d_ResNet_checkpoint.pth.tar' % (epoch + 1)\n",
    "    torch.save(state, './checkpoint/' + filename)\n",
    "\n",
    "def main():\n",
    "\n",
    "    '''\n",
    "    Training and Validation\n",
    "    '''\n",
    "\n",
    "    global checkpoint, start_epoch\n",
    "\n",
    "    if checkpoint is None:\n",
    "\n",
    "        model = AHCR(num_classes=15)\n",
    "\n",
    "        optimizer = torch.optim.SGD(params=filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                     lr=learning_rate)\n",
    "\n",
    "    else:\n",
    "\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        model = checkpoint['model']\n",
    "        optimizer = checkpoint['optimizer']\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = BCELoss().to(device)\n",
    "\n",
    "    with open(train_annotation_path) as f:\n",
    "        train_lines = f.readlines()\n",
    "\n",
    "    train_loader = DataLoader(data_loader(train_lines, input_shape), batch_size=batch_size, shuffle=True,\n",
    "                               drop_last=True, collate_fn=DvX_dataset_collate)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        if epoch is not 0 and epoch % 10 == 0:\n",
    "            adjust_learning_rate(optimizer, 0.1)\n",
    "\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        save_checkpoint(epoch, model, optimizer)\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    Loss = []\n",
    "\n",
    "    ap_meter = AveragePrecisionMeter()\n",
    "    ap_meter.reset()\n",
    "\n",
    "    for i, (img_ols, img_sds, gt_s) in enumerate(train_loader):\n",
    "\n",
    "        img_ols = img_ols.to(device)\n",
    "        img_sds = img_sds.to(device)\n",
    "        gt_s = gt_s.to(device)\n",
    "\n",
    "        ol_output, sd_output = model(img_ols, img_sds)\n",
    "\n",
    "        loss = criterion(ol_output, sd_output, gt_s)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        Loss.append(loss.item())\n",
    "\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i is not 0 and i % print_freq == 0:\n",
    "            print('Epoch: [{}]/[{}/{}]\\t'\n",
    "                  'Loss: {:.3f}'.format((epoch + 1), i, len(train_loader),\n",
    "                                        sum(Loss)/len(Loss),))\n",
    "\n",
    "        prediction = confidence_weighted_view_fusion(torch.sigmoid(ol_output), torch.sigmoid(sd_output))\n",
    "\n",
    "        ap_meter.add(prediction.data, gt_s)\n",
    "\n",
    "    each_ap = ap_meter.value()\n",
    "    map = 100 * each_ap.mean()\n",
    "\n",
    "    print(each_ap)\n",
    "    print('mAP: {:.3f}'.format(map))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: xray_resnet18_cls.pth\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     60\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✔ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# predict(\"checkpoint/ep030_ResNet_checkpoint.pth.tar\", \"P00000\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mxray_resnet18_cls.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mP00000\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mpredict\u001b[39m\u001b[34m(checkpoint_path, image_id)\u001b[39m\n\u001b[32m     31\u001b[39m torch.serialization.add_safe_globals([AHCR])\n\u001b[32m     33\u001b[39m checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m model = \u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.to(device)\n\u001b[32m     35\u001b[39m model.eval()\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# ---- Load images ----\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'model'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from model_ResNet import AHCR\n",
    "from utils import confidence_weighted_view_fusion\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_shape = (224, 224)\n",
    "\n",
    "classes = ['Gun', 'Knife', 'Wrench', 'Pliers', 'Scissors', 'Lighter',\n",
    "           'Battery', 'Bat', 'Razor_blade', 'Saw_blade', 'Fireworks',\n",
    "           'Hammer', 'Screwdriver', 'Dart', 'Pressure_vessel']\n",
    "\n",
    "\n",
    "def load_img(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, input_shape)\n",
    "    img = img.astype(np.float32) / 255.\n",
    "\n",
    "    # Convert 1-channel → 3-channel for ResNet50\n",
    "    img = np.stack([img, img, img], axis=0)   # shape: (3, 224, 224)\n",
    "\n",
    "    return torch.tensor(img, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "\n",
    "def predict(checkpoint_path, image_id):\n",
    "    print(\"Loading checkpoint:\", checkpoint_path)\n",
    "\n",
    "    # allow AHCR class during loading\n",
    "    torch.serialization.add_safe_globals([AHCR])\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    model = checkpoint[\"model\"].to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # ---- Load images ----\n",
    "    ol_path = f\"./data/classification/val/{image_id}_OL.png\"\n",
    "    sd_path = f\"./data/classification/val/{image_id}_SD.png\"\n",
    "\n",
    "    img_ol = load_img(ol_path).to(device)\n",
    "    img_sd = load_img(sd_path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ol_out, sd_out = model(img_ol, img_sd)\n",
    "\n",
    "    ol_prob = torch.sigmoid(ol_out)\n",
    "    sd_prob = torch.sigmoid(sd_out)\n",
    "\n",
    "    fused = confidence_weighted_view_fusion(ol_prob, sd_prob)\n",
    "    fused = fused.squeeze().cpu().numpy()\n",
    "\n",
    "    print(\"\\n===== Prediction =====\")\n",
    "    for cls, score in zip(classes, fused):\n",
    "        print(f\"{cls:15s}: {score:.4f}\")\n",
    "\n",
    "    print(\"\\nDetected:\")\n",
    "    for cls, score in zip(classes, fused):\n",
    "        if score > 0.5:\n",
    "            print(f\"✔ {cls}: {score:.3f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict(\"checkpoint/ep030_ResNet_checkpoint.pth.tar\", \"P00000\")\n",
    "    # predict(\"xray_resnet18_cls.pth\", \"P00000\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
