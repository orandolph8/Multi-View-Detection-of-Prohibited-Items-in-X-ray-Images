{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DvXray Full Processing Pipeline  \n",
    "**Author:** Soon-Hyuck Lee  \n",
    "**Dataset:** Dual-View X-ray Baggage Detection (DvXray)\n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. Loads raw DvXray dataset (Positive + Negative)\n",
    "2. Creates train/val split\n",
    "3. Generates YOLO dataset (images + labels)\n",
    "4. Generates COCO dataset (images + annotations)\n",
    "5. Generates Classification dataset (images + .npy label vectors)\n",
    "6. Handles dual-view OL/SD images\n",
    "7. Handles “difficult” and missing bounding boxes\n",
    "8. Supports negative samples\n",
    "\n",
    "Run each cell in order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Base directories\n",
    "BASE = \"./data\"\n",
    "RAW_POS = f\"{BASE}/raw/DvXray_Positive_Samples\"\n",
    "RAW_NEG = f\"{BASE}/raw/DvXray_Negative_Samples\"\n",
    "\n",
    "SPLIT_DIR = f\"{BASE}/processed/classification/split\"\n",
    "YOLO_OUT = f\"{BASE}/processed/yolo\"\n",
    "COCO_OUT = f\"{BASE}/processed/coco\"\n",
    "CLS_OUT  = f\"{BASE}/processed/classification\"\n",
    "\n",
    "IMG_W = 800\n",
    "IMG_H = 600\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "random.seed(19)\n",
    "\n",
    "prohibited_item_classes = {\n",
    "    'Gun': 0, 'Knife': 1, 'Wrench': 2, 'Pliers': 3, 'Scissors': 4,\n",
    "    'Lighter': 5, 'Battery': 6, 'Bat': 7, 'Razor_blade': 8,\n",
    "    'Saw_blade': 9, 'Fireworks': 10, 'Hammer': 11,\n",
    "    'Screwdriver': 12, 'Dart': 13, 'Pressure_vessel': 14\n",
    "}\n",
    "\n",
    "print(\"Config loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility: Normalize BBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_bbox(bb, json_path, view_name):\n",
    "    \"\"\"Normalize bbox into [x1,y1,x2,y2] or return None.\"\"\"\n",
    "\n",
    "    if bb is None or bb == \"difficult\":\n",
    "        return None\n",
    "    if bb == []:\n",
    "        return None\n",
    "    if isinstance(bb, list) and len(bb) == 1 and isinstance(bb[0], list):\n",
    "        bb = bb[0]\n",
    "    if len(bb) > 4:\n",
    "        print(f\"[WARN] {json_path}: {view_name} has {len(bb)} values → truncating.\")\n",
    "        bb = bb[:4]\n",
    "    if len(bb) != 4:\n",
    "        print(f\"[WARN] {json_path}: {view_name} INVALID ({bb}) → skipping.\")\n",
    "        return None\n",
    "    return bb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility: Classification Label Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_vector(json_path):\n",
    "    \"\"\"Return multi-hot 15-dim vector.\"\"\"\n",
    "    data = json.load(open(json_path))\n",
    "    objs = data[\"objects\"]\n",
    "\n",
    "    if objs == \"None\":\n",
    "        return np.zeros(15, dtype=np.int32)\n",
    "\n",
    "    arr = np.zeros(15, dtype=np.int32)\n",
    "    for obj in objs:\n",
    "        idx = prohibited_item_classes[obj[\"label\"]]\n",
    "        arr[idx] = 1\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split complete. Total: 16000, Train: 12800, Val: 3200\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(SPLIT_DIR, exist_ok=True)\n",
    "\n",
    "all_ids = []\n",
    "for folder in [RAW_POS, RAW_NEG]:\n",
    "    for f in os.listdir(folder):\n",
    "        if f.endswith(\".json\"):\n",
    "            all_ids.append(f.replace(\".json\", \"\"))\n",
    "\n",
    "all_ids.sort()\n",
    "n = len(all_ids)\n",
    "k = int(n * TRAIN_RATIO)\n",
    "train_ids = set(random.sample(all_ids, k))\n",
    "\n",
    "with open(f\"{SPLIT_DIR}/train.txt\", \"w\") as f:\n",
    "    for sid in all_ids:\n",
    "        if sid in train_ids:\n",
    "            f.write(sid + \"\\n\")\n",
    "\n",
    "with open(f\"{SPLIT_DIR}/val.txt\", \"w\") as f:\n",
    "    for sid in all_ids:\n",
    "        if sid not in train_ids:\n",
    "            f.write(sid + \"\\n\")\n",
    "\n",
    "print(f\"Split complete. Total: {n}, Train: {k}, Val: {n-k}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: YOLO Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO functions loaded.\n"
     ]
    }
   ],
   "source": [
    "for split in [\"train\", \"val\"]:\n",
    "    os.makedirs(f\"{YOLO_OUT}/images/{split}\", exist_ok=True)\n",
    "    os.makedirs(f\"{YOLO_OUT}/labels/{split}\", exist_ok=True)\n",
    "\n",
    "def convert_to_yolo(json_path, ol_label, sd_label):\n",
    "    data = json.load(open(json_path))\n",
    "    objs = data[\"objects\"]\n",
    "\n",
    "    f_ol = open(ol_label, \"w\")\n",
    "    f_sd = open(sd_label, \"w\")\n",
    "\n",
    "    if objs == \"None\":\n",
    "        f_ol.close()\n",
    "        f_sd.close()\n",
    "        return\n",
    "\n",
    "    for obj in objs:\n",
    "        cls_id = prohibited_item_classes[obj[\"label\"]]\n",
    "\n",
    "        # OL\n",
    "        ol = normalize_bbox(obj[\"ol_bb\"], json_path, \"ol_bb\")\n",
    "        if ol is not None:\n",
    "            x1, y1, x2, y2 = ol\n",
    "            cx = (x1 + x2) / 2 / IMG_W\n",
    "            cy = (y1 + y2) / 2 / IMG_H\n",
    "            w  = (x2 - x1) / IMG_W\n",
    "            h  = (y2 - y1) / IMG_H\n",
    "            f_ol.write(f\"{cls_id} {cx} {cy} {w} {h}\\n\")\n",
    "\n",
    "        # SD\n",
    "        sd = normalize_bbox(obj[\"sd_bb\"], json_path, \"sd_bb\")\n",
    "        if sd is not None:\n",
    "            x1, y1, x2, y2 = sd\n",
    "            cx = (x1 + x2) / 2 / IMG_W\n",
    "            cy = (y1 + y2) / 2 / IMG_H\n",
    "            w  = (x2 - x1) / IMG_W\n",
    "            h  = (y2 - y1) / IMG_H\n",
    "            f_sd.write(f\"{cls_id} {cx} {cy} {w} {h}\\n\")\n",
    "\n",
    "    f_ol.close()\n",
    "    f_sd.close()\n",
    "\n",
    "print(\"YOLO functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run YOLO Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating YOLO dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12800/12800 [00:20<00:00, 612.55it/s]\n",
      "100%|██████████| 3200/3200 [00:05<00:00, 593.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO dataset complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating YOLO dataset...\")\n",
    "\n",
    "for split in [\"train\", \"val\"]:\n",
    "    ids = open(f\"{SPLIT_DIR}/{split}.txt\").read().split()\n",
    "    for sid in tqdm(ids):\n",
    "        folder = RAW_POS if os.path.exists(f\"{RAW_POS}/{sid}.json\") else RAW_NEG\n",
    "        json_path = f\"{folder}/{sid}.json\"\n",
    "\n",
    "        shutil.copy(f\"{folder}/{sid}_OL.png\", f\"{YOLO_OUT}/images/{split}\")\n",
    "        shutil.copy(f\"{folder}/{sid}_SD.png\", f\"{YOLO_OUT}/images/{split}\")\n",
    "\n",
    "        convert_to_yolo(\n",
    "            json_path,\n",
    "            f\"{YOLO_OUT}/labels/{split}/{sid}_OL.txt\",\n",
    "            f\"{YOLO_OUT}/labels/{split}/{sid}_SD.txt\"\n",
    "        )\n",
    "\n",
    "print(\"YOLO dataset complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: COCO Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\", \"val\"]:\n",
    "    os.makedirs(f\"{COCO_OUT}/{split}\", exist_ok=True)\n",
    "os.makedirs(f\"{COCO_OUT}/annotations\", exist_ok=True)\n",
    "\n",
    "def coco_generate(split_name):\n",
    "\n",
    "    img_id = 1\n",
    "    ann_id = 1\n",
    "\n",
    "    images = []\n",
    "    annotations = []\n",
    "\n",
    "    ids = open(f\"{SPLIT_DIR}/{split_name}.txt\").read().split()\n",
    "    out_img_dir = f\"{COCO_OUT}/{split_name}\"\n",
    "\n",
    "    for sid in tqdm(ids):\n",
    "        folder = RAW_POS if os.path.exists(f\"{RAW_POS}/{sid}.json\") else RAW_NEG\n",
    "        json_path = f\"{folder}/{sid}.json\"\n",
    "        data = json.load(open(json_path))\n",
    "        objs = data[\"objects\"]\n",
    "\n",
    "        for view in [\"OL\", \"SD\"]:\n",
    "\n",
    "            fname = f\"{sid}_{view}.png\"\n",
    "            shutil.copy(f\"{folder}/{fname}\", out_img_dir)\n",
    "\n",
    "            images.append({\n",
    "                \"id\": img_id,\n",
    "                \"file_name\": fname,\n",
    "                \"width\": IMG_W,\n",
    "                \"height\": IMG_H\n",
    "            })\n",
    "\n",
    "            if objs != \"None\":\n",
    "                for obj in objs:\n",
    "                    cls = prohibited_item_classes[obj[\"label\"]]\n",
    "                    bb = obj[\"ol_bb\"] if view == \"OL\" else obj[\"sd_bb\"]\n",
    "                    bb = normalize_bbox(bb, json_path, view)\n",
    "                    if bb is None:\n",
    "                        continue\n",
    "                    x1, y1, x2, y2 = bb\n",
    "                    w = x2 - x1\n",
    "                    h = y2 - y1\n",
    "\n",
    "                    annotations.append({\n",
    "                        \"id\": ann_id,\n",
    "                        \"image_id\": img_id,\n",
    "                        \"category_id\": cls,\n",
    "                        \"bbox\": [x1, y1, w, h],\n",
    "                        \"area\": w * h,\n",
    "                        \"iscrowd\": 0\n",
    "                    })\n",
    "                    ann_id += 1\n",
    "\n",
    "            img_id += 1\n",
    "\n",
    "    coco_dict = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": [\n",
    "            {\"id\": cid, \"name\": name}\n",
    "            for name, cid in prohibited_item_classes.items()\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(f\"{COCO_OUT}/annotations/instances_{split_name}.json\", \"w\") as f:\n",
    "        json.dump(coco_dict, f, indent=4)\n",
    "\n",
    "    print(f\"COCO {split_name} done: {len(images)} images, {len(annotations)} annotations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12800/12800 [00:53<00:00, 237.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO train done: 25600 images, 8406 annotations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3200/3200 [00:21<00:00, 145.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO val done: 6400 images, 2177 annotations.\n",
      "COCO dataset complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "coco_generate(\"train\")\n",
    "coco_generate(\"val\")\n",
    "print(\"COCO dataset complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12800/12800 [00:17<00:00, 736.91it/s]\n",
      "100%|██████████| 3200/3200 [00:05<00:00, 602.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification dataset complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for split in [\"train\", \"val\"]:\n",
    "    os.makedirs(f\"{CLS_OUT}/{split}\", exist_ok=True)\n",
    "\n",
    "for split in [\"train\", \"val\"]:\n",
    "    ids = open(f\"{SPLIT_DIR}/{split}.txt\").read().split()\n",
    "    out_dir = f\"{CLS_OUT}/{split}\"\n",
    "\n",
    "    for sid in tqdm(ids):\n",
    "        folder = RAW_POS if os.path.exists(f\"{RAW_POS}/{sid}.json\") else RAW_NEG\n",
    "        json_path = f\"{folder}/{sid}.json\"\n",
    "\n",
    "        lbl = get_label_vector(json_path)\n",
    "        np.save(f\"{out_dir}/{sid}.npy\", lbl)\n",
    "\n",
    "        shutil.copy(f\"{folder}/{sid}_OL.png\", out_dir)\n",
    "        shutil.copy(f\"{folder}/{sid}_SD.png\", out_dir)\n",
    "\n",
    "print(\"Classification dataset complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Completed Successfully!\n",
    "\n",
    "### ✔ YOLO dataset  \n",
    "- data/processed/yolo/images/train  \n",
    "- data/processed/yolo/labels/train  \n",
    "\n",
    "### ✔ COCO dataset  \n",
    "- data/processed/coco/train  \n",
    "- instances_train.json  \n",
    "\n",
    "### ✔ Classification dataset  \n",
    "- data/processed/classification/train  \n",
    "- .npy multi-label files  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
